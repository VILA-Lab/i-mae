# A PyTorch Implementation of i-MAE: Linearly Separable Representation in MAE
[Kevin Zhang*](https://kzyz.netlify.com/), [Zhiqiang Shen*](http://zhiqiangshen.com/)


[`Website`](https://zhiqiangshen.com/projects/i-mae/) | [`arXiv`](https://arxiv.org/abs/) | [`BibTeX`](#citation)


[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github)  

![i-MAE2](https://user-images.githubusercontent.com/52997677/196735725-496592d3-5883-4db4-ba34-04d0a8dab535.svg)

We provide a PyTorch/GPU based implementation of our technical report [i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable?
]()

### Catalog
- [x] Pretrain demo with Colab
- [ ] More code coming soon!




### Visualization demo

Please visit our [website](https://zhiqiangshen.com/projects/i-mae/), or run our visualization demo with [Colab notebook](https://colab.research.google.com/github/facebookresearch/mae/blob/main/demo/mae_visualize.ipynb) (no GPU needed):

### Acknowledgement

This repository is based on [timm](https://github.com/rwightman/pytorch-image-models/tree/master/timm) and [MAE](https://github.com/facebookresearch/mae) repositories.

### License

This project is under the CC-BY-NC 4.0 license. See [LICENSE](LICENSE) for details.

### Citation

If you find this repository helpful, please consider citing our work:

```
@article{zhang2022i-mae,
  author    = {Zhang, Kevin and Shen, Zhiqiang},
  title     = {i-MAE: Are Latent Representations in Masked Autoencoders Linearly Separable?},
  journal   = {arxiv preprint},
  year      = {2022},
}
```
